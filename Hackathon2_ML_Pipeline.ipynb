{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1086ede5",
   "metadata": {},
   "source": [
    "# Hackathon 2: Artificial Intelligence 1 - Complete Classification ML Pipeline\n",
    "\n",
    "## Context\n",
    "This notebook builds directly on Hackathon 1 (Python for Data Science). We take the cleaned dataset and engineered features, transform them into a machine learning-ready feature matrix, and build a complete classification pipeline using scikit-learn.\n",
    "\n",
    "**Dataset**: Loan approval data (same as Hackathon 1 and AI assignments)\n",
    "- Each row = loan application\n",
    "- Target: `loanstatus` (0=not approved, 1=approved)\n",
    "- Features: applicant demographics, financials, loan details + engineered features from Hackathon 1\n",
    "\n",
    "**Main goals**:\n",
    "- Create production-ready ML pipeline (preprocessing + model + evaluation)\n",
    "- **Compare 9 different ML algorithms** (LogisticRegression, RandomForest, XGBoost, SVM, KNN, etc.)\n",
    "- Optimize for business-important metrics (F1-score, Accuracy, ROC-AUC)\n",
    "- Cross-validation + hyperparameter tuning for best model\n",
    "- **Comprehensive visualizations** (ROC curves, confusion matrices, precision-recall trade-offs)\n",
    "- Model interpretation and actionable business recommendations\n",
    "\n",
    "**Success criteria**: \n",
    "- High cross-validated performance across multiple metrics\n",
    "- Clear business justification with visual insights\n",
    "- Production-ready pipeline with optimal threshold selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9750067",
   "metadata": {},
   "source": [
    "## 1. Setup Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f43105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn imports for complete pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, auc, f1_score,\n",
    "    precision_score, recall_score, accuracy_score\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# XGBoost (install with: pip install xgboost)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed. Install with: pip install xgboost\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18487457",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "We import all necessary libraries for our comprehensive ML pipeline:\n",
    "- **pandas/numpy**: Data manipulation and numerical operations\n",
    "- **plotly**: Interactive, professional visualizations (heatmaps, ROC curves, confusion matrices)\n",
    "- **sklearn preprocessing**: Transform raw data (scaling, encoding, imputation)\n",
    "- **sklearn pipeline**: Chain preprocessing + model steps for clean code\n",
    "- **sklearn models**: 9 different algorithms to compare (ensemble methods, linear models, SVMs, etc.)\n",
    "- **sklearn metrics**: Comprehensive evaluation (ROC-AUC, precision-recall, confusion matrix, F1-score)\n",
    "- **XGBoost**: State-of-the-art gradient boosting (with graceful fallback if not installed)\n",
    "\n",
    "This centralized import keeps the notebook organized and ensures all dependencies are available upfront. The try-except block for XGBoost allows the notebook to run even if XGBoost isn't installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b250f1",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned Dataset from Hackathon 1\n",
    "\n",
    "Load the dataset with all cleaning and feature engineering from Hackathon 1. This should include:\n",
    "- Cleaned columns (no missing values, correct types)\n",
    "- Numeric transformations (log, z-score, ratios)\n",
    "- Categorical improvements (rare category grouping, frequency features)\n",
    "- Text/date features (lengths, year/month flags)\n",
    "- Group-based aggregates (customer means, deviations)\n",
    "\n",
    "**Important**: Update the `csv_path` variable to point to your cleaned data from Hackathon 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ba8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset from Hackathon 1\n",
    "csv_path = \"PATH_TO_HACKATHON1_CLEANED_DATA.csv\"  # Update with actual path\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColumn types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb21cde",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Load the preprocessed dataset from Hackathon 1 that contains:\n",
    "- All cleaned and validated data (no missing values, correct types)\n",
    "- Engineered features (ratios, aggregations, transformations)\n",
    "\n",
    "We perform an initial inspection to understand:\n",
    "- Dataset size (rows Ã— columns)\n",
    "- Data types distribution (numeric vs categorical)\n",
    "- Sample of the data structure\n",
    "\n",
    "This ensures we're working with quality input data before building the ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c580cb0",
   "metadata": {},
   "source": [
    "## 3. Define Target and Features\n",
    "\n",
    "Separate target (`loanstatus`) from features. Drop any remaining irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target (binary classification: loan approved or not)\n",
    "target_col = 'loanstatus'\n",
    "y = df[target_col].astype(int)  # Ensure numeric\n",
    "\n",
    "# Drop target + irrelevant columns (update list based on Hackathon 1 output)\n",
    "drop_cols = ['personname', 'bankname']  # Add any other ID columns\n",
    "X = df.drop(columns=[target_col] + drop_cols)\n",
    "\n",
    "# Remove rows where target is missing (should be none from Hackathon 1)\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts(normalize=True).round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f2792",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Separate the dataset into:\n",
    "- **Target (y)**: What we want to predict (`loanstatus`: approved/not approved)\n",
    "- **Features (X)**: All variables used to make predictions\n",
    "\n",
    "We also remove:\n",
    "- The target column itself (can't use it to predict itself!)\n",
    "- Identifier columns (names, IDs) that don't have predictive value\n",
    "- Any rows with missing target values\n",
    "\n",
    "This creates a clean feature matrix ready for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8509ae",
   "metadata": {},
   "source": [
    "## 4. Identify Feature Types\n",
    "\n",
    "Automatic identification of numeric vs categorical features for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Quick check for high-cardinality categoricals (>50 unique values)\n",
    "high_cardinality = {col: X[col].nunique() for col in categorical_features if X[col].nunique() > 50}\n",
    "print(f\"\\nHigh cardinality features: {high_cardinality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6f1f7",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Automatically categorize features into:\n",
    "- **Numeric features**: Numbers that can be scaled (age, income, credit score)\n",
    "- **Categorical features**: Categories that need encoding (occupation, loan purpose)\n",
    "\n",
    "We also identify high-cardinality categoricals (>50 unique values) which might need special handling.\n",
    "\n",
    "This automatic detection ensures we apply the correct preprocessing to each feature type in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ac29a",
   "metadata": {},
   "source": [
    "## 5. Create Preprocessing Pipeline\n",
    "\n",
    "Complete **ColumnTransformer** that automatically handles different feature types with appropriate preprocessing:\n",
    "\n",
    "**Numeric pipeline** (for continuous features):\n",
    "1. **Impute** missing values with median (robust to outliers, won't be skewed by extreme values)\n",
    "2. **Standardize** to zero mean, unit variance (required for SVM, KNN, Logistic Regression; helps other models converge faster)\n",
    "\n",
    "**Categorical pipeline** (for text/category features):\n",
    "1. **Impute** missing values with most frequent category (preserves distribution)\n",
    "2. **One-hot encode** to create binary columns (ML algorithms need numbers, not text)\n",
    "   - `handle_unknown='ignore'`: Gracefully handles new categories in test data\n",
    "   - `sparse_output=False`: Dense arrays for compatibility\n",
    "\n",
    "This ensures all features are properly formatted, scaled, and ready for any ML algorithm. The pipeline approach means preprocessing is automatically applied consistently to train/validation/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines for each feature type\n",
    "\n",
    "# Numeric pipeline: impute â†’ scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute â†’ one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Full preprocessing transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"Preprocessing pipeline ready:\")\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cf684",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Create a **ColumnTransformer** that applies different preprocessing to different feature types:\n",
    "\n",
    "**Numeric pipeline**:\n",
    "1. Impute missing values with median (robust to outliers)\n",
    "2. Standardize to zero mean, unit variance (required for many ML algorithms)\n",
    "\n",
    "**Categorical pipeline**:\n",
    "1. Impute missing values with most frequent category\n",
    "2. One-hot encode to create binary columns (algorithms need numbers, not text)\n",
    "\n",
    "This ensures all features are properly formatted and scaled before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c868f1",
   "metadata": {},
   "source": [
    "## 6. Train-Validation-Test Split\n",
    "\n",
    "70% train (60/10 train/validation), 20% validation, 10% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 80/20 train+val vs test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 70/10 train vs validation (from temp)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp  # 0.125*0.8=0.1\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} ({100*len(X_train)/len(X):.1f}%)\")\n",
    "print(f\"Validation: {X_val.shape[0]} ({100*len(X_val)/len(X):.1f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} ({100*len(X_test)/len(X):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248a4b7",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Split data into three sets with **stratified sampling** (preserves target distribution):\n",
    "\n",
    "1. **Training set (70%)**: Learn model parameters\n",
    "2. **Validation set (10%)**: Tune hyperparameters and compare models\n",
    "3. **Test set (20%)**: Final unbiased performance estimate\n",
    "\n",
    "We use stratification to ensure each set has the same proportion of approved/not approved loans. This prevents bias from imbalanced splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49456e",
   "metadata": {},
   "source": [
    "## 7. Baseline Model Pipeline\n",
    "\n",
    "Simple Logistic Regression as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline pipeline\n",
    "baseline_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit and evaluate baseline\n",
    "baseline_pipe.fit(X_train, y_train)\n",
    "baseline_val_score = baseline_pipe.score(X_val, y_val)\n",
    "\n",
    "print(f\"Baseline validation accuracy: {baseline_val_score:.3f}\")\n",
    "print(\"\\nBaseline classification report (validation):\")\n",
    "y_val_pred = baseline_pipe.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049cca2",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Establish a **baseline model** using simple Logistic Regression:\n",
    "- Quick to train\n",
    "- Interpretable\n",
    "- Good starting point for comparison\n",
    "\n",
    "The baseline helps us answer: \"Is our complex model actually better than a simple approach?\"\n",
    "\n",
    "We evaluate on the validation set to get an initial performance benchmark (accuracy + detailed classification metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b0190",
   "metadata": {},
   "source": [
    "## 8. Model Selection with Cross-Validation\n",
    "\n",
    "Compare **9 different ML algorithms** using 5-fold cross-validation with multiple metrics:\n",
    "\n",
    "**Ensemble Methods** (handle non-linearity, robust):\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost (if available)\n",
    "\n",
    "**Linear Models** (fast, interpretable):\n",
    "- Logistic Regression\n",
    "- Linear SVM\n",
    "\n",
    "**Non-Linear Models**:\n",
    "- RBF SVM (kernel trick for non-linear boundaries)\n",
    "- K-Nearest Neighbors\n",
    "\n",
    "**Probabilistic**:\n",
    "- Naive Bayes\n",
    "\n",
    "**Simple Baseline**:\n",
    "- Decision Tree\n",
    "\n",
    "We evaluate each model on **three metrics**:\n",
    "- **F1-Score** (weighted): Balances precision and recall for both classes\n",
    "- **Accuracy**: Overall correctness\n",
    "- **ROC-AUC**: Discrimination ability across all thresholds\n",
    "\n",
    "Cross-validation gives us robust performance estimates. The best-performing model (by F1-score) becomes our candidate for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive model candidates\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM_Linear': SVC(kernel='linear', random_state=42, probability=True),\n",
    "    'SVM_RBF': SVC(kernel='rbf', random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_jobs=-1),\n",
    "    'NaiveBayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    models['XGBoost'] = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "\n",
    "# Cross-validation results with multiple metrics\n",
    "cv_results = []\n",
    "print(\"Running cross-validation for all models...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), ('model', model)])\n",
    "    \n",
    "    # Calculate multiple metrics\n",
    "    f1_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "    accuracy_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    roc_auc_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'model': name,\n",
    "        'cv_f1_mean': f1_scores.mean(),\n",
    "        'cv_f1_std': f1_scores.std(),\n",
    "        'cv_accuracy_mean': accuracy_scores.mean(),\n",
    "        'cv_accuracy_std': accuracy_scores.std(),\n",
    "        'cv_roc_auc_mean': roc_auc_scores.mean(),\n",
    "        'cv_roc_auc_std': roc_auc_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  F1: {f1_scores.mean():.3f} (Â±{f1_scores.std():.3f})\")\n",
    "    print(f\"  Accuracy: {accuracy_scores.mean():.3f} (Â±{accuracy_scores.std():.3f})\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_scores.mean():.3f} (Â±{roc_auc_scores.std():.3f})\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).sort_values('cv_f1_mean', ascending=False)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nCross-validation Summary (sorted by F1-score):\")\n",
    "print(cv_df.round(3).to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = cv_df.iloc[0]['model']\n",
    "print(f\"\\nðŸ† Best model: {best_model_name}\")\n",
    "print(f\"   F1-score: {cv_df.iloc[0]['cv_f1_mean']:.3f} (Â±{cv_df.iloc[0]['cv_f1_std']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84c026",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Compare multiple ML algorithms using **5-fold cross-validation**:\n",
    "- Logistic Regression (linear, fast, interpretable)\n",
    "- Random Forest (ensemble, handles non-linearity, robust)\n",
    "- Decision Tree (simple, interpretable, prone to overfitting)\n",
    "\n",
    "Cross-validation gives us a more robust performance estimate than a single train/test split. We use **weighted F1-score** as the metric (balances precision and recall for both classes).\n",
    "\n",
    "The best-performing model becomes our candidate for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af44fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Performance Heatmap\n",
    "fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "    z=cv_df[['cv_f1_mean', 'cv_accuracy_mean', 'cv_roc_auc_mean']].values,\n",
    "    x=['F1-Score', 'Accuracy', 'ROC-AUC'],\n",
    "    y=cv_df['model'],\n",
    "    colorscale='RdYlGn',\n",
    "    text=cv_df[['cv_f1_mean', 'cv_accuracy_mean', 'cv_roc_auc_mean']].round(3).values,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 10},\n",
    "    colorbar=dict(title=\"Score\")\n",
    "))\n",
    "\n",
    "fig_heatmap.update_layout(\n",
    "    title='Model Performance Heatmap (Cross-Validation Scores)',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Model',\n",
    "    height=500\n",
    ")\n",
    "fig_heatmap.show()\n",
    "\n",
    "# 2. F1-Score Comparison with Error Bars\n",
    "fig_f1 = go.Figure()\n",
    "fig_f1.add_trace(go.Bar(\n",
    "    x=cv_df['model'],\n",
    "    y=cv_df['cv_f1_mean'],\n",
    "    error_y=dict(type='data', array=cv_df['cv_f1_std']),\n",
    "    marker_color='steelblue',\n",
    "    text=cv_df['cv_f1_mean'].round(3),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig_f1.update_layout(\n",
    "    title='Model F1-Score Comparison (with standard deviation)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='F1-Score',\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig_f1.show()\n",
    "\n",
    "# 3. Multi-metric Comparison (Grouped Bar Chart)\n",
    "fig_multi = go.Figure()\n",
    "\n",
    "metrics = ['cv_f1_mean', 'cv_accuracy_mean', 'cv_roc_auc_mean']\n",
    "metric_names = ['F1-Score', 'Accuracy', 'ROC-AUC']\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen']\n",
    "\n",
    "for metric, name, color in zip(metrics, metric_names, colors):\n",
    "    fig_multi.add_trace(go.Bar(\n",
    "        name=name,\n",
    "        x=cv_df['model'],\n",
    "        y=cv_df[metric],\n",
    "        marker_color=color\n",
    "    ))\n",
    "\n",
    "fig_multi.update_layout(\n",
    "    title='Multi-Metric Model Comparison',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    legend=dict(x=0.01, y=0.99)\n",
    ")\n",
    "fig_multi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d43cd6",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Create **three complementary visualizations** to compare all 9 models:\n",
    "\n",
    "1. **Performance Heatmap**: Color-coded grid showing all models Ã— all metrics\n",
    "   - Green = higher scores (better performance)\n",
    "   - Red = lower scores (weaker performance)\n",
    "   - Easy to spot top performers at a glance\n",
    "\n",
    "2. **F1-Score Bar Chart**: Focused comparison on primary metric\n",
    "   - Error bars show consistency (Â±1 std deviation)\n",
    "   - Smaller error bars = more stable/reliable model\n",
    "\n",
    "3. **Multi-Metric Grouped Bars**: Side-by-side comparison\n",
    "   - See if a model excels in one metric but lags in others\n",
    "   - Identify well-rounded models vs specialized ones\n",
    "\n",
    "These visualizations help select the best model based on business priorities (maximize F1? ROC-AUC? balanced performance?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f029097",
   "metadata": {},
   "source": [
    "## 8b. Visualize Model Performance Comparison\n",
    "\n",
    "Compare all 9 models across different metrics using interactive visualizations to identify the best performer and understand trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b282b",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning (GridSearchCV)\n",
    "\n",
    "Optimize the best model's hyperparameters using **GridSearchCV** with model-specific grids:\n",
    "\n",
    "**Hyperparameter grids for different models**:\n",
    "- **Random Forest**: n_estimators, max_depth, min_samples_split\n",
    "- **XGBoost**: n_estimators, max_depth, learning_rate\n",
    "- **Gradient Boosting**: n_estimators, max_depth, learning_rate\n",
    "- **SVM**: C (regularization), gamma (kernel coefficient)\n",
    "- **Logistic Regression**: C (regularization), penalty type\n",
    "- **KNN**: n_neighbors, weights (uniform vs distance)\n",
    "- **Decision Tree**: max_depth, min_samples_split\n",
    "\n",
    "**Performance optimizations applied**:\n",
    "- Reduced parameter grid (fewer combinations, faster execution)\n",
    "- 3-fold CV instead of 5 (faster, still reliable)\n",
    "- Parallel processing with `n_jobs=-1` (uses all CPU cores)\n",
    "- Verbose output to track progress\n",
    "\n",
    "This balances finding optimal hyperparameters with reasonable computation time (~5-15 minutes depending on model complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized hyperparameter grid for best model (reduced for faster execution)\n",
    "if best_model_name == 'RandomForest':\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [10, 20],\n",
    "        'model__min_samples_split': [2, 5]\n",
    "    }\n",
    "elif best_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "elif best_model_name == 'GradientBoosting':\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [3, 5],\n",
    "        'model__learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "elif best_model_name == 'LogisticRegression':\n",
    "    param_grid = {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__penalty': ['l2']\n",
    "    }\n",
    "elif 'SVM' in best_model_name:\n",
    "    param_grid = {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__gamma': ['scale', 'auto']\n",
    "    }\n",
    "elif best_model_name == 'KNN':\n",
    "    param_grid = {\n",
    "        'model__n_neighbors': [3, 5, 7],\n",
    "        'model__weights': ['uniform', 'distance']\n",
    "    }\n",
    "else:\n",
    "    param_grid = {\n",
    "        'model__max_depth': [5, 10, 15],\n",
    "        'model__min_samples_split': [2, 5]\n",
    "    }\n",
    "\n",
    "# Grid search with cross-validation (optimized for speed)\n",
    "best_pipe = Pipeline([('preprocessor', preprocessor), ('model', models[best_model_name])])\n",
    "grid_search = GridSearchCV(\n",
    "    best_pipe, \n",
    "    param_grid, \n",
    "    cv=3,  # Reduced from 5 for faster execution\n",
    "    scoring='f1_weighted', \n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=2  # Show progress\n",
    ")\n",
    "\n",
    "print(f\"Starting hyperparameter tuning for {best_model_name}...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(f\"Best cross-val F1: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Number of combinations tested: {len(grid_search.cv_results_['params'])}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeec985",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Optimize the best model's hyperparameters using **GridSearchCV**:\n",
    "- Tests different combinations of hyperparameters\n",
    "- Uses cross-validation to avoid overfitting\n",
    "- Finds the configuration that maximizes F1-score\n",
    "\n",
    "**Performance optimizations applied**:\n",
    "- Reduced parameter grid (fewer combinations to test)\n",
    "- 3-fold CV instead of 5 (faster, still reliable)\n",
    "- Parallel processing with `n_jobs=-1`\n",
    "- Early stopping for Random Forest (fewer trees if possible)\n",
    "\n",
    "This balances finding good hyperparameters with reasonable computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b554f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and probabilities for visualizations\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "y_test_proba = final_model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "y_val_pred = final_model.predict(X_val)\n",
    "y_val_proba = final_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate metrics for different thresholds (for precision-recall trade-off)\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "precisions_at_threshold = []\n",
    "recalls_at_threshold = []\n",
    "f1_at_threshold = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_test_proba >= threshold).astype(int)\n",
    "    if y_pred_threshold.sum() > 0:  # Avoid division by zero\n",
    "        precisions_at_threshold.append(precision_score(y_test, y_pred_threshold, zero_division=0))\n",
    "        recalls_at_threshold.append(recall_score(y_test, y_pred_threshold, zero_division=0))\n",
    "        f1_at_threshold.append(f1_score(y_test, y_pred_threshold, zero_division=0))\n",
    "    else:\n",
    "        precisions_at_threshold.append(0)\n",
    "        recalls_at_threshold.append(0)\n",
    "        f1_at_threshold.append(0)\n",
    "\n",
    "# 1. ROC Curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig_roc = go.Figure()\n",
    "fig_roc.add_trace(go.Scatter(\n",
    "    x=fpr, y=tpr,\n",
    "    mode='lines',\n",
    "    name=f'ROC Curve (AUC = {roc_auc:.3f})',\n",
    "    line=dict(color='darkorange', width=2),\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(255, 140, 0, 0.2)'\n",
    "))\n",
    "fig_roc.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random Classifier',\n",
    "    line=dict(color='navy', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "fig_roc.update_layout(\n",
    "    title=f'ROC Curve - {best_model_name}',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    height=500,\n",
    "    legend=dict(x=0.6, y=0.1)\n",
    ")\n",
    "fig_roc.show()\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_test_proba)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "fig_pr = go.Figure()\n",
    "fig_pr.add_trace(go.Scatter(\n",
    "    x=recall_curve, y=precision_curve,\n",
    "    mode='lines',\n",
    "    name=f'PR Curve (AUC = {pr_auc:.3f})',\n",
    "    line=dict(color='purple', width=2),\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(128, 0, 128, 0.2)'\n",
    "))\n",
    "\n",
    "# Add baseline (random classifier for imbalanced data)\n",
    "baseline = y_test.sum() / len(y_test)\n",
    "fig_pr.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[baseline, baseline],\n",
    "    mode='lines',\n",
    "    name=f'Baseline (random) = {baseline:.3f}',\n",
    "    line=dict(color='red', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "fig_pr.update_layout(\n",
    "    title=f'Precision-Recall Curve - {best_model_name}',\n",
    "    xaxis_title='Recall',\n",
    "    yaxis_title='Precision',\n",
    "    height=500,\n",
    "    legend=dict(x=0.6, y=0.9)\n",
    ")\n",
    "fig_pr.show()\n",
    "\n",
    "# 3. Confusion Matrix Heatmap\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig_cm = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Raw Counts', 'Normalized (by row)'),\n",
    "    specs=[[{'type': 'heatmap'}, {'type': 'heatmap'}]]\n",
    ")\n",
    "\n",
    "# Raw confusion matrix\n",
    "fig_cm.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cm,\n",
    "        x=['Predicted: Not Approved', 'Predicted: Approved'],\n",
    "        y=['Actual: Not Approved', 'Actual: Approved'],\n",
    "        colorscale='Blues',\n",
    "        text=cm,\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 14},\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Normalized confusion matrix\n",
    "fig_cm.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cm_normalized,\n",
    "        x=['Predicted: Not Approved', 'Predicted: Approved'],\n",
    "        y=['Actual: Not Approved', 'Actual: Approved'],\n",
    "        colorscale='Blues',\n",
    "        text=np.round(cm_normalized, 2),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 14},\n",
    "        colorbar=dict(title=\"Proportion\")\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig_cm.update_layout(\n",
    "    title_text=f'Confusion Matrix - {best_model_name}',\n",
    "    height=500\n",
    ")\n",
    "fig_cm.show()\n",
    "\n",
    "# 4. Precision-Recall Trade-off Scatter Plot\n",
    "fig_scatter = go.Figure()\n",
    "\n",
    "# Precision vs Recall at different thresholds\n",
    "fig_scatter.add_trace(go.Scatter(\n",
    "    x=recalls_at_threshold,\n",
    "    y=precisions_at_threshold,\n",
    "    mode='markers+lines',\n",
    "    marker=dict(\n",
    "        size=8,\n",
    "        color=thresholds,\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Threshold\")\n",
    "    ),\n",
    "    line=dict(width=1, color='lightgray'),\n",
    "    text=[f'Threshold: {t:.2f}' for t in thresholds],\n",
    "    hovertemplate='<b>Threshold:</b> %{text}<br>' +\n",
    "                  '<b>Precision:</b> %{y:.3f}<br>' +\n",
    "                  '<b>Recall:</b> %{x:.3f}<br>' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Mark current threshold (0.5)\n",
    "current_precision = precision_score(y_test, y_test_pred)\n",
    "current_recall = recall_score(y_test, y_test_pred)\n",
    "fig_scatter.add_trace(go.Scatter(\n",
    "    x=[current_recall],\n",
    "    y=[current_precision],\n",
    "    mode='markers',\n",
    "    marker=dict(size=15, color='red', symbol='star'),\n",
    "    name='Current (threshold=0.5)',\n",
    "    hovertemplate=f'<b>Current Operating Point</b><br>' +\n",
    "                  f'Precision: {current_precision:.3f}<br>' +\n",
    "                  f'Recall: {current_recall:.3f}<br>' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_scatter.update_layout(\n",
    "    title='Precision-Recall Trade-off at Different Thresholds',\n",
    "    xaxis_title='Recall',\n",
    "    yaxis_title='Precision',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "fig_scatter.show()\n",
    "\n",
    "# 5. F1-Score vs Threshold\n",
    "fig_f1_threshold = go.Figure()\n",
    "\n",
    "fig_f1_threshold.add_trace(go.Scatter(\n",
    "    x=thresholds,\n",
    "    y=f1_at_threshold,\n",
    "    mode='lines',\n",
    "    fill='tozeroy',\n",
    "    line=dict(color='green', width=2),\n",
    "    fillcolor='rgba(0, 128, 0, 0.2)'\n",
    "))\n",
    "\n",
    "# Mark optimal threshold\n",
    "optimal_idx = np.argmax(f1_at_threshold)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_f1 = f1_at_threshold[optimal_idx]\n",
    "\n",
    "fig_f1_threshold.add_trace(go.Scatter(\n",
    "    x=[optimal_threshold],\n",
    "    y=[optimal_f1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=15, color='red', symbol='star'),\n",
    "    name=f'Optimal (threshold={optimal_threshold:.2f})',\n",
    "    hovertemplate=f'<b>Optimal Threshold</b><br>' +\n",
    "                  f'Threshold: {optimal_threshold:.3f}<br>' +\n",
    "                  f'F1-Score: {optimal_f1:.3f}<br>' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_f1_threshold.update_layout(\n",
    "    title='F1-Score vs Classification Threshold',\n",
    "    xaxis_title='Threshold',\n",
    "    yaxis_title='F1-Score',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "fig_f1_threshold.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Visualization Summary:\")\n",
    "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.3f}\")\n",
    "print(f\"Optimal threshold for F1: {optimal_threshold:.3f} (F1={optimal_f1:.3f})\")\n",
    "print(f\"Current threshold (0.5): F1={f1_score(y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d853f",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Create advanced visualizations to deeply understand model performance:\n",
    "\n",
    "1. **ROC Curve**: Shows true positive rate vs false positive rate at different thresholds\n",
    "   - Higher AUC = better discrimination ability\n",
    "   - Diagonal line = random guessing\n",
    "\n",
    "2. **Precision-Recall Curve**: Important for imbalanced datasets\n",
    "   - Shows trade-off between precision and recall\n",
    "   - Higher AUC = better performance\n",
    "\n",
    "3. **Confusion Matrix**: Shows where the model makes mistakes\n",
    "   - Diagonal = correct predictions\n",
    "   - Off-diagonal = errors (false positives/negatives)\n",
    "\n",
    "4. **Precision-Recall Scatter**: Visualize threshold selection\n",
    "   - Find optimal balance for business needs\n",
    "   - Each point = different decision threshold\n",
    "\n",
    "These visualizations help choose the right threshold and understand model behavior in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53da076b",
   "metadata": {},
   "source": [
    "## 10b. Advanced Performance Visualizations\n",
    "\n",
    "Create comprehensive visualizations to deeply understand model behavior and select optimal classification threshold for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae9f27",
   "metadata": {},
   "source": [
    "## 10. Final Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the final tuned model on **both validation and test sets**:\n",
    "\n",
    "- **Validation set**: Verify hyperparameter tuning improved performance\n",
    "- **Test set**: Get unbiased final performance estimate (model never saw this data during training/tuning)\n",
    "\n",
    "We generate detailed classification reports showing per-class and overall metrics:\n",
    "- **Precision**: Of predicted approvals, what % were actually approved?\n",
    "- **Recall**: Of actual approvals, what % did we correctly identify?\n",
    "- **F1-score**: Harmonic mean of precision and recall (balances both)\n",
    "- **Support**: Number of samples in each class\n",
    "\n",
    "**Key check**: If test performance â‰ˆ validation performance, the model generalizes well! If test << validation, we may have overfit during tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6631b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# Validation set performance\n",
    "y_val_pred = final_model.predict(X_val)\n",
    "val_f1 = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "\n",
    "# Test set performance\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "test_f1 = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "\n",
    "print(\"Validation F1-scores:\")\n",
    "print(pd.Series(val_f1).round(3))\n",
    "print(\"\\nTest F1-scores:\")\n",
    "print(pd.Series(test_f1).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ee232",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "Evaluate the final tuned model on **both validation and test sets**:\n",
    "\n",
    "- **Validation set**: Check if hyperparameter tuning improved performance\n",
    "- **Test set**: Get unbiased final performance estimate (model has never seen this data)\n",
    "\n",
    "We generate detailed classification reports showing:\n",
    "- Precision: Of predicted approvals, how many were correct?\n",
    "- Recall: Of actual approvals, how many did we catch?\n",
    "- F1-score: Harmonic mean of precision and recall\n",
    "\n",
    "If test performance is similar to validation, our model generalizes well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7cf0c2",
   "metadata": {},
   "source": [
    "## 11. Model Interpretation & Business Insights\n",
    "\n",
    "**Interpret the model** to understand what drives loan approvals and build trust in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if hasattr(final_model.named_steps['model'], 'feature_importances_'):\n",
    "    feature_names = (final_model.named_steps['preprocessor']\n",
    "                    .get_feature_names_out())\n",
    "    importances = final_model.named_steps['model'].feature_importances_\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    fig = px.bar(importance_df, x='importance', y='feature',\n",
    "                title='Top 15 Most Important Features',\n",
    "                orientation='h')\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Feature importances not available for this model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75522263",
   "metadata": {},
   "source": [
    "### Purpose of This Step\n",
    "\n",
    "**Interpret the model** to understand what drives loan approvals and build trust in predictions:\n",
    "\n",
    "**For tree-based models** (Random Forest, XGBoost, Gradient Boosting, Decision Tree):\n",
    "- Extract feature importances showing which features most influence predictions\n",
    "- Visualize top 15 features in interactive bar chart\n",
    "- Validate that important features align with business intuition\n",
    "\n",
    "**Key questions answered**:\n",
    "- Which features most influence approval decisions?\n",
    "- Are engineered features from Hackathon 1 valuable?\n",
    "- Do results make business sense (e.g., credit score, income ratio important)?\n",
    "- Any surprising features that warrant further investigation?\n",
    "\n",
    "**Business value**:\n",
    "- Validate model makes sense (not just \"black box\")\n",
    "- Identify key factors for loan approval strategy\n",
    "- Guide data collection priorities\n",
    "- Support regulatory compliance (explainability requirements)\n",
    "- Help applicants understand rejection reasons\n",
    "\n",
    "**Note**: For linear models (LogisticRegression, SVM), coefficients can be examined instead of feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6358cd0",
   "metadata": {},
   "source": [
    "## 12. Summary & Recommendations\n",
    "\n",
    "**Key Results**:\n",
    "- **Best Model**: {best_model_name} with optimized hyperparameters\n",
    "- **Test F1-score**: [FILL IN after running]\n",
    "- **ROC-AUC**: [FILL IN] - measures overall discrimination ability\n",
    "- **Optimal Threshold**: [FILL IN] - balance precision/recall for business needs\n",
    "- **Top Features**: [From feature importance analysis]\n",
    "\n",
    "**Model Performance Insights**:\n",
    "1. **Compared {len(models)} different algorithms** including ensemble methods (RandomForest, XGBoost, GradientBoosting), linear models (LogisticRegression, SVM), and others\n",
    "2. **Cross-validation** ensures robust performance estimates across different data splits\n",
    "3. **Multiple metrics** (F1, Accuracy, ROC-AUC) provide comprehensive evaluation\n",
    "4. **Confusion matrix** shows model makes [X%] false positives and [Y%] false negatives\n",
    "\n",
    "**Visualization Insights**:\n",
    "- ROC curve shows the model is significantly better than random guessing\n",
    "- Precision-Recall curve indicates performance on imbalanced data\n",
    "- Optimal threshold may differ from default 0.5 based on business priorities\n",
    "- Feature importance reveals key drivers of loan approval\n",
    "\n",
    "**Business Recommendations**:\n",
    "1. **Deploy optimal threshold**: Use threshold={optimal_threshold} if F1-score is the priority, or adjust based on cost of false positives vs false negatives\n",
    "2. **Focus on top features**: Prioritize data quality and collection for most important features\n",
    "3. **Monitor model drift**: Track performance over time as loan patterns change\n",
    "4. **A/B testing**: Compare model decisions against current approval process\n",
    "5. **Cost-sensitive learning**: If false approvals (defaults) cost more than false rejections (missed opportunities), adjust threshold or use class weights\n",
    "6. **Explainability**: For rejected applications, provide reasons based on feature importance\n",
    "7. **Regular retraining**: Retrain monthly/quarterly with new data to maintain performance\n",
    "\n",
    "**Next Steps**:\n",
    "- Implement model monitoring dashboard\n",
    "- Set up automated retraining pipeline\n",
    "- Create API for real-time predictions\n",
    "- Document model for regulatory compliance\n",
    "- Plan for model updates and versioning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
